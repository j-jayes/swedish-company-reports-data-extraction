---
title: "Extracting schematic information from tabular PDF documents using a large language model pipeline"
subtitle: "23-01-2024 - Machine Learning in Economic History Workshop"
author: 
  - "Jonathan Jayes  - Lund University"
title-slide-attributes:
  data-background-image: "assets/robot.png"
  data-background-position: "bottom 10"
  data-background-size: "90%"
  data-background-opacity: "0.4"
format: 
  html:
    code-block-bg: true
    code-block-border-left: "#31BAE9"
    include-in-header: 
        - text: <link rel = "shortcut icon" href = "assets/favicon.ico" />
image: "assets/robot.png"
favicon: "assets/favicon.ico"
execute: 
  echo: false
  eval: true
  message: false
---


# Motivation{#sec-motivation}

::: {.columns}
::: {.column}
What do we as economic historians love?

![Lovely long time series](assets/gdp-per-capita-maddison.png)

:::
::: {.column}
What do we hate?

![Lots of time spent typing things into excel](assets/wikihow.jpeg)

:::
:::

## What is possible with off-the-shelf OCR tech?

![A table that can be OCRd relatively easily](assets/ocr_good.png)

- Computer written text
- Clean lines separating rows and columnss
- Single heading per column

## What does off-the-shelf OCR tech struggle with?

![A table that is difficult to OCR](assets/ocr_hard.jpg)

- Poor quality scans
- Lacks clean lines separating rows
- Multiple headings per column

## Conventional solutions to theses problems

![[ABBYY FineReader](https://bendingwater-blog.library.claremont.edu/2021/04/21/a-look-inside-the-data-ocr-process-and-challenging-tables/)](assets/orc_hard_solution.jpg)

- Clicky, fiddly, GUI based software
- Expensive
- Not clear that time invested improves system

## Custom machine learning solutions

![[Reading the ransom: Methodological advancements in extracting the Swedish Wealth Tax of 1571]()](assets/reading_ransom.jpg)

- Use machine learning to train a model to recognize specific tables
- Fantastic for old documents which would be impossible to OCR otherwise
- May not generalize well to other documents

## Custom machine learning solutions

![[Layout Parser: open-source deep-learning powered library for automatically processing document image data at scale](https://dell-research-harvard.github.io/resources/layout-parser)](assets/layout_parser.png)

- Use machine learning to train a model to recognize specific tables
- Good for when you have a specific type of document you want to extract data from that doesn't change much over time
- 'Open source' - many open GitHub issues
- You really need to know about deep learning and computer vision to use this well.

# Research question{#sec-researchquestion background-color="midnightblue" visibilty="uncounted"}

How can we use a multi-modal machine learning model to extract tabular data from historical documents of varying vintages?

## Example use case:

Company reports in Sweden.

I showcase the method using the annual reports of **Electrolux**, a Swedish company founded in 1919, now the world's second largest appliance maker by units sold.

<br>

![1937 Vacuum Cleaner](assets/vacuum.jpg)

## Source of data

![[Annual Reports for companies listed at the Stockholm Stock Exchange during the period 1912 to 1978](https://www.hhs.se/en/houseoffinance/data-center/historical-archives/annual-reports-archive/)](assets/source_material.png)

## How do the reports change over time?

```{r}
library(tidyverse)
theme_set(theme_light())

df = readr::read_csv("data/companies/Electrolux/pages.csv")

# rename column 1 year
df = df %>% rename(year = ...1)

df = df %>% 
mutate(language = case_when
(year < 1950 ~ "Swedish",
TRUE ~ "English"))


df %>% 
ggplot(aes(x = year, y = pages)) +
geom_line(group = 1, colour = "midnightblue") +
geom_point(aes(colour = language)) +
scale_colour_brewer(palette = "Dark2") +
labs(x = "Year of report",
y = "Number of pages in report",
colour = "Report written in")
```

## Reports over time

::: {layout-ncol=3}

![1925](assets/1925_crop.png)

![1950](assets/Electrolux_1950_page_5.jpeg)

![1975](assets/Electrolux_1975_page_11.jpeg)

:::


## Reports over time: GRAPHIC DESIGN!

::: {layout-ncol=3}

![1957](assets/Electrolux_1957_page_1.jpeg)

![1972](assets/Electrolux_1972_page_1.jpeg)

![1984](assets/Electrolux_1984_page_1.jpeg)

:::

# Approach{#sec-approach background-color="midnightblue"}

Combination of vector database for document search and multi-modal machine learning for table extraction.

## Approach schematic

![How we get information out of tabular PDFs](assets/mermaid.png)

## Semantic search explained

![Search process](assets/parent-document-retriever.png)

## Use text embeddings to represent chunks as vectors{background-color="#1b9e77"}

<br>

"Vinst- och förlusträkning" ~ [1, 0, 1, 0, 0, ...] ~ "Profit and loss account"

<br>

Then we find the page of the document that contains the Profit and Loss account.

<br>

::: {.columns}
::: {.column}

```{r}
library(tidyverse)
df = readxl::read_excel("data/companies/Electrolux/extract_pages.xlsx")

df %>% 
ggplot(aes(x = year, y = p_and_l)) +
geom_point(colour = "midnightblue") +
labs(x = "Year of report",
 y = "Page number of \nprofit and loss account") +
 theme(    text = element_text(size = 20)
)

```

:::
::: {.column}

*The National Library of Sweden / KBLab released three pretrained language models based on BERT. The models are trained on approximately 15-20GB of text (200M sentences, 3000M tokens) from various sources (books, news, government publications, swedish wikipedia and internet forums) aiming to provide a representative BERT model for Swedish text.*^[[KB Lab Swedish BERT Models](https://huggingface.co/KB/bert-base-swedish-cased)]

:::
:::

## Search through chunks for P and L statement{background-color="#d95f02"}

![](assets/Electrolux_1960_page_13.png){width=100%}

## Pass the page to GPT-4 Vision with Schema{background-color="#e7298a"}

```{python}
#| echo: true
#| eval: false

schema = [
        {
            "key": "year",
            "description": "The year for which the financial statement is being reported."
        },
        {
            "key": "taxes",
            "description": "Total taxes paid by the company, also called 'skatt' in Swedish"
        },
        {
            "key": "net_profit",
            "description": "Net profit earned by the company for the year, also called 'Nettovinst för året' in Swedish"
        }
]

payload = {
    "model": "gpt-4-vision-preview",
    "messages": [
    {
        "role": "user",
        "content": [
        {"type": "text", "text": prompt},
        {
            "type": "image_url",
            "image_url": {
            "url": f"data:image/jpeg;base64,{base64_image}"
            }
        }
        ]
    }
    ],
    "max_tokens": 1000
}

```

## System output

![Electrolux Net Profit and Taxes](assets/figures.png)



## Why does this work well? What are the limitations?

:::: {.columns}
::: {.column}
**Advantages**

**Skip step structuring with OCR**

- This is a difficult task
- 'Small' errors can lead to big problems
- E.G 1,00 vs 1.00 or leading zeros

**'Read' the document like a research assistant**

- 'Understand' the context of the tables

**Fast and cheap**

- 2 mins processing time to get the result for 75 years of reports
- After you've built the pipeline, switching to another company is easy

:::
::: {.column}
**Limitations**

- Requires clever schema design to get the data you want
- Requires time to manually check outlier results
- Works best with small tables
- Not easily reproducible by others (though neither are research assistants)

:::
::::

# Conclusion{#sec-conclusion background-color="midnightblue"}
